{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b6675b-e802-4bbd-b1bd-6af8e0906af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da99c4f1-d453-4b16-b741-ce8edf5bc240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available devices: 1\n",
      "current device: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "print(f'available devices: {torch.cuda.device_count()}')\n",
    "print(f'current device: { torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c3780d-15df-436f-b7f1-5cf26157d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForTokenClassification, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042427d0-4a4a-4dec-9911-56cb175efad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained(\"tner/roberta-large-conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf4ad5e-78f8-4e9b-8985-fb88b90fbb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tner/roberta-large-conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97caf31f-591b-4873-ac03-144e30402281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration gtfintechlab--finer-ord-f945a026865ebcab\n",
      "Found cached dataset csv (/home/ottowg/.cache/huggingface/datasets/gtfintechlab___csv/gtfintechlab--finer-ord-f945a026865ebcab/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1705d20e4adb455187e8a07636586009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finer_ord = datasets.load_dataset('gtfintechlab/finer-ord')\n",
    "\n",
    "label_to_label_idx = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-LOC': 3, 'I-LOC': 4, 'B-ORG': 5, 'I-ORG': 6}\n",
    "label_idx_to_label = {v:k for k,v in label_to_label_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a81c86-0388-4ed6-b0ca-8eb9e613dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "83792\n",
      "validation\n",
      "10635\n",
      "test\n",
      "27031\n"
     ]
    }
   ],
   "source": [
    "def save_as_connl(finer_ord_orig, part, target_filename):\n",
    "    print(part)\n",
    "    connl = []\n",
    "    last_sent, last_doc = 0, 0\n",
    "    for i in finer_ord[part]:\n",
    "        if i[\"sent_idx\"] != last_sent or i[\"doc_idx\"] != last_doc:\n",
    "            connl.append(\"\\n\")\n",
    "        label = label_idx_to_label[i[\"gold_label\"]]\n",
    "        if i[\"gold_token\"] is None:\n",
    "            print(f\"Why is this token None? {i}\")\n",
    "            i[\"gold_token\"] = \"\"\n",
    "        token = i['gold_token'].strip()\n",
    "        if token:\n",
    "            connl.append(f\"{token} {label}\")\n",
    "        last_sent, last_doc = i[\"sent_idx\"], i[\"doc_idx\"]\n",
    "    print(len(connl))\n",
    "    connl = \"\\n\".join(connl)\n",
    "    with open(target_filename, \"w\") as f:\n",
    "        f.write(connl)\n",
    "save_as_connl(finer_ord, \"train\", \"data/train.txt\")\n",
    "save_as_connl(finer_ord, \"validation\", \"data/validation.txt\")\n",
    "save_as_connl(finer_ord, \"test\", \"data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e407f5-1654-4009-b86b-fa35d90dd450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 'B-LOC',\n",
       " 2: 'B-MISC',\n",
       " 1: 'B-ORG',\n",
       " 3: 'B-PER',\n",
       " 8: 'I-LOC',\n",
       " 7: 'I-MISC',\n",
       " 6: 'I-ORG',\n",
       " 4: 'I-PER',\n",
       " 0: 'O'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id_roberta = {'B-LOC': 5, 'B-MISC': 2, 'B-ORG': 1, 'B-PER': 3, 'I-LOC': 8, 'I-MISC': 7, 'I-ORG': 6, 'I-PER': 4, 'O': 0}\n",
    "id2label_roberta = {v:k for k, v in label2id_roberta.items()}\n",
    "id2label_roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1178ca9b-9216-4763-8239-90716c817d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(batch, id2label):\n",
    "    token = tokenizer(text=batch, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    pred = np.argmax(model(**token).logits.detach().numpy(), axis=2)[:,1:-1]\n",
    "    word_ids_batch = [token.word_ids(i)[1:-1] for i in range(len(token.input_ids))]\n",
    "    word_ids_batch = [[w if w is not None else len(b) - 1 for w in wb]\n",
    "                      for wb, b in zip(word_ids_batch, batch)]\n",
    "    word_ids_batch = np.array(word_ids_batch)\n",
    "    word_ids_batch_next = np.roll(word_ids_batch, shift=1, axis=1)\n",
    "    word_ids_batch_next[:,0] = -1\n",
    "    mask_first_token = word_ids_batch != word_ids_batch_next\n",
    "    mask_attention = token[\"attention_mask\"].numpy().astype(\"bool\")[:,1:-1]\n",
    "    mask_first_token = mask_first_token & mask_attention\n",
    "    pred_tag_idxs = np.ma.masked_array(pred, mask=~mask_first_token)\n",
    "    to_label = np.vectorize(id2label.get, otypes=[\"O\"])\n",
    "    tags = np.apply_along_axis(to_label, 0, pred_tag_idxs).astype('U').tolist()\n",
    "    tags = [[t for t in s if t is not None] for s in tags]\n",
    "    for s, t in zip(batch, tags):\n",
    "        assert len(s) == len(t)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a2ac410-39d3-4c58-be76-1b327bdb56b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'B-LOC',\n",
       "  'B-LOC',\n",
       "  'B-LOC',\n",
       "  'B-LOC',\n",
       "  'B-LOC'],\n",
       " ['O', 'O', 'B-LOC', 'I-LOC', 'I-LOC'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC'],\n",
       " ['O'],\n",
       " []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\n",
    "    [\"hello\", \"here\",\"and\", \"Thomas\", \"Obama\", \"no\", \"no\", \"London\", \"London\", \"London\", \"London\", \"London\", \"London\"],\n",
    "    [\"hi\", \"here\", \"Paris\", \"and\", \"London\"],\n",
    "    ['Tue', ',', 'Oct', '20', ',', '2015', ',', '03:57', 'BST', '-', 'UK'],\n",
    "    ['â€¦'],\n",
    "    [],\n",
    "]\n",
    "#for b in batch:\n",
    "#    print(len(b))\n",
    "expected = [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG']]\n",
    "predict_batch(batch, id2label_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a31f99f7-8c89-45bd-b5cb-96eca8f4c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load connl format into sentences\n",
    "fn = \"data/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cefe632-cf1b-4efe-a6e1-4095edc5f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1075, 1075)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(fn) as f:\n",
    "    lines = f.readlines()\n",
    "    sentences = [[]]\n",
    "    sentences_label = [[]]\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.strip():\n",
    "            word, label = line.strip().split()\n",
    "            sentences[-1].append(word)\n",
    "            sentences_label[-1].append(label)\n",
    "        elif sentences and len(sentences[-1]) != 0:#if idx + 1 < len(lines):\n",
    "            sentences.append([])\n",
    "            sentences_label.append([])\n",
    "len(sentences), len(sentences_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "203b05f0-4adf-435f-81b4-2e15c63e73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences[351], sentences_label[351]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f936b675-5023-4f3f-831d-ff2c6f768315",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_roberta_wo_misc = {\n",
    " 5: 'B-LOC',\n",
    " 2: 'O', # 'B-MISC',\n",
    " 1: 'B-ORG',\n",
    " 3: 'B-PER',\n",
    " 8: 'I-LOC',\n",
    " 7: 'O', # 'I-MISC',\n",
    " 6: 'I-ORG',\n",
    " 4: 'I-PER',\n",
    " 0: 'O'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05267d2b-88aa-4ec5-a534-58e6e4a75a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "start = 0\n",
    "current_batch = None\n",
    "predictions = []\n",
    "while current_batch or not predictions:\n",
    "    current_batch = sentences[:None][start:start+batch_size]\n",
    "    if current_batch:\n",
    "        prediction = predict_batch(current_batch, id2label_roberta_wo_misc)\n",
    "        predictions.extend(prediction)\n",
    "    print(f\"\\r{len(predictions)}\", end=\"\")\n",
    "    start += batch_size\n",
    "#predict_batch(sentences[100:103], id2label_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9c8390c-8b4e-47c7-9d47-a0cb60165de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "metric = evaluate.load('seqeval')\n",
    "\n",
    "def compute(predictions, references):\n",
    "    performance = metric.compute(predictions=predictions, references=references)\n",
    "    micro = pd.Series({k[8:]: v for k, v in performance.items() if k.startswith(\"overall_\")})\n",
    "    label_performance = {k: v for k, v in performance.items() if not k.startswith(\"overall_\")}    \n",
    "    metrics_df = pd.DataFrame(label_performance).T\n",
    "    weights = metrics_df.number.divide(metrics_df.number.sum())\n",
    "    weighted_average_macro = metrics_df[[\"precision\", \"recall\", \"f1\"]].multiply(weights, axis=0).sum()\n",
    "    metrics_df.loc[\"micro\"] = micro\n",
    "    metrics_df.loc[\"macro\"] = metrics_df[[\"precision\", \"recall\", \"f1\"]].mean()\n",
    "    metrics_df.loc[\"macro_weighted\"] = weighted_average_macro\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85b48715-b67b-4955-8b03-6201cc7b35aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.740299</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.781102</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.673721</td>\n",
       "      <td>0.690778</td>\n",
       "      <td>0.682143</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.908451</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>0.748735</td>\n",
       "      <td>0.779631</td>\n",
       "      <td>0.763871</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro</th>\n",
       "      <td>0.767801</td>\n",
       "      <td>0.799793</td>\n",
       "      <td>0.783095</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_weighted</th>\n",
       "      <td>0.750197</td>\n",
       "      <td>0.779631</td>\n",
       "      <td>0.764233</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                precision    recall        f1  number\n",
       "LOC              0.740299  0.826667  0.781102   300.0\n",
       "ORG              0.673721  0.690778  0.682143   553.0\n",
       "PER              0.908451  0.902098  0.905263   286.0\n",
       "micro            0.748735  0.779631  0.763871     NaN\n",
       "macro            0.767801  0.799793  0.783095     NaN\n",
       "macro_weighted   0.750197  0.779631  0.764233     NaN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute(predictions, sentences_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383c05d-86c6-4bfc-8bd0-0d6bf9dad897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_basic",
   "language": "python",
   "name": "ml_basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
